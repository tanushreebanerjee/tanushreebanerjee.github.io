<!DOCTYPE html>
<meta charset="utf-8" emacsmode="-*- markdown -*-">
<link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/journal.css?">
<style>
/* Burgundy color theme override */
.md h1, .md h2, .md h3, .md h4, .md h5, .md h6 { color: #8B1538; }
.md a { color: #8B1538; }
.md a:hover { color: #A0446C; }
</style>

<!-- Markdeep: https://casual-effects.com/markdeep/ -->

# Optical Flow

**Module 4.1** | [‚Üê Back to Index](../index.html) | [Next: Feature Matching ‚Üí](04-temporal-matching.md.html)

---

## Overview

Optical flow: Dense pixel-level motion between consecutive frames. Estimates 2D motion vectors indicating where each pixel moves.

---

## Essential Papers

### üìñ FlowNet (2015)

**FlowNet: Learning Optical Flow with Convolutional Networks** (Dosovitskiy et al.) | [arXiv](https://arxiv.org/abs/1504.06852)

**Key idea**: First end-to-end deep learning for optical flow.

**Architecture**: Two-stream CNN (one for each image), correlation layer, refinement.

**Limitations**: Lower accuracy than traditional methods, slow.

---

### FlowNet 2.0 (2017)

**FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks** (Ilg et al.) | [arXiv](https://arxiv.org/abs/1612.01925)

**Improvements**:
- Stack multiple FlowNet modules
- Warping between stages
- Better training strategy

**Benefits**: Better accuracy, still slower than traditional.

---

### üî• PWC-Net (2018)

**PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume** (Sun et al.) | [arXiv](https://arxiv.org/abs/1709.02371)

**Key components**:
- **Pyramid**: Multi-scale feature extraction
- **Warping**: Warp features from previous scale
- **Cost volume**: Compute matching costs
- **Refinement**: Iterative refinement

**Benefits**: Better accuracy than FlowNet, more efficient.

---

### üî• RAFT (2020)

**RAFT: Recurrent All-Pairs Field Transforms for Optical Flow** (Teed & Deng) | [arXiv](https://arxiv.org/abs/2003.12039)

**Key innovation**: Recurrent update mechanism for iterative refinement.

**Architecture**:
1. Feature extraction from both images
2. Correlation pyramid (all-pairs correlations)
3. Recurrent GRU updates flow field iteratively
4. Upsampling to full resolution

**Benefits**:
- State-of-the-art accuracy
- Generalizes well (fewer domain gaps)
- Efficient inference

[Figure placeholder: RAFT architecture diagram]

---

### GMFlow (2022)

**GMFlow: Learning Optical Flow via Global Matching** (Xu et al.) | [arXiv](https://arxiv.org/abs/2111.13680)

**Key idea**: Global matching instead of local correlation.

**Benefits**: Better handling of large motions, occlusions.

---

## Core Concepts

### Optical Flow Problem

**Goal**: For each pixel in frame 1, find corresponding pixel in frame 2.

**Brightness constancy**: Assumes pixel intensity doesn't change: $I(x, y, t) = I(x+u, y+v, t+1)$

**Aperture problem**: Only normal flow can be determined from local patches.

**Solution**: Require smoothness or use global constraints.

---

### Cost Volume

**Definition**: 3D volume encoding matching costs for each pixel at different displacements.

**Construction**: For each pixel, compute similarity with pixels at different offsets.

**Usage**: Find best matching displacement (argmin over offsets).

**Memory**: Large memory requirement (H √ó W √ó displacement_range).

---

### Iterative Refinement

**Coarse-to-fine**: Start with low resolution, refine at higher resolutions.

**Warping**: Warp second image using current flow estimate, compute residual flow.

**Iterative updates**: Update flow field iteratively using GRU or CNN.

---

## Problems Solved by Optical Flow

### Dense Motion Estimation
**Problem**: Track motion of every pixel between frames. Needed for video understanding, editing, analysis.

**Optical flow solution**: Dense pixel-level motion vectors. Every pixel has motion estimate, enabling fine-grained motion understanding.

[Figure placeholder: Visualization showing input frames and dense optical flow field with color-coded motion vectors]

### Video Understanding Foundation
**Problem**: Understanding video requires understanding motion. Static features insufficient.

**Solution**: Optical flow provides motion features. Foundation for action recognition, video segmentation, video understanding tasks.

### Real-Time Motion Tracking
**Problem**: Track objects or regions across video frames. Needed for applications like video editing, surveillance, analysis.

**RAFT solution**: Real-time or near-real-time optical flow enables interactive applications. Fast enough for real-time video processing.

### Handling Large Motions
**Problem**: Large object motions or camera motion can cause traditional methods to fail.

**Learned methods solution**: Deep learning methods (RAFT, GMFlow) handle large motions better than traditional methods. Learned features more robust.

---

## Remaining Challenges and Limitations

### Occlusions
**Problem**: When objects move behind other objects, optical flow is ambiguous. Which object should pixel belong to?

**Current solutions**: Some methods model occlusions explicitly, but remains challenging.

**Open question**: Better occlusion handling? Explicit occlusion reasoning?

### Textureless Regions
**Problem**: Regions with little texture (plain walls, sky) make optical flow ambiguous. Can't determine motion reliably.

**Remaining**: Challenging for traditional and learned methods. Often requires global constraints or assumptions.

### Lighting Changes
**Problem**: Changes in lighting between frames violate brightness constancy assumption. Can cause errors.

**Open question**: Better handling of illumination changes? Adaptive brightness models?

### Computational Cost
**Problem**: High-resolution optical flow can be computationally expensive, especially for real-time applications.

**Current solutions**: Efficient architectures, optimization, but trade-offs remain.

**Open question**: Better efficiency without quality loss?

### Ground Truth Availability
**Problem**: Collecting dense optical flow ground truth is expensive. Limited training data.

**Open question**: Better synthetic data? Self-supervised methods? Few-shot learning?

### Extreme Motions
**Problem**: Very fast motion or motion blur can cause failures.

**Remaining**: Challenging cases where motion exceeds model's training distribution.

---

## Broader Insights and Implications

### Learning from Video
**Insight**: Optical flow demonstrates that motion is rich signal for learning. Many self-supervised methods use optical flow or motion as supervision signal.

**Broader impact**: Shows value of temporal information. Motion provides strong learning signal. Influences design of video understanding methods.

### The Evolution from Hand-Crafted to Learned
**Insight**: Optical flow shows evolution: traditional methods (Lucas-Kanade, Horn-Schunck) ‚Üí learned features (FlowNet) ‚Üí end-to-end learning (RAFT). Learned methods now dominate.

**Broader impact**: Demonstrates that deep learning can improve even well-established classical problems. Encourages revisiting classical CV with modern methods.

### Real-Time Requirements Drive Innovation
**Insight**: Need for real-time optical flow (video processing, robotics) drives development of efficient architectures.

**Broader impact**: Shows how application requirements (real-time) influence research directions. Efficiency becomes as important as accuracy.

### The Importance of Benchmarks
**Insight**: Standard benchmarks (Sintel, KITTI) enable fair comparison and rapid progress.

**Broader impact**: Demonstrates value of standardized evaluation. Enables tracking progress and identifying bottlenecks.

### Motion as Fundamental Signal
**Insight**: Motion (optical flow) is fundamental to understanding video, dynamic scenes, temporal processes.

**Broader impact**: Highlights that motion understanding is core to many vision tasks. Influences design of video understanding systems.

[Placeholder for manual expansion: Add insights about connections to biology (human vision), applications in robotics, video processing industry]

---

## Applications

- Video stabilization
- Motion segmentation
- Action recognition
- SLAM (DROID-SLAM uses optical flow)
- Video interpolation
- Scene flow (3D motion)

[Figure placeholder: Applications diagram showing optical flow used in video stabilization, action recognition, SLAM, video editing, etc.]

---

## Related Modules

- Module 4.2: Feature Matching (sparse correspondence)
- Module 4.3: Scene Flow (3D motion)
- Module 3.4: SLAM (uses optical flow)

---

## Additional Resources

- **RAFT Repository**: Official implementation
- **Optical Flow Benchmarks**: Sintel, KITTI, FlyingThings3D

---

---
[‚Üê Back to Index](../index.html) | [Next: Feature Matching ‚Üí](04-temporal-matching.md.html)
---


<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="markdeep.min.js" charset="utf-8"></script><script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js" charset="utf-8"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
