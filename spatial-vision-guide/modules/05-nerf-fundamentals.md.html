<!DOCTYPE html>
<meta charset="utf-8" emacsmode="-*- markdown -*-">
<link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/journal.css?">
<style>
/* Burgundy color theme override */
.md h1, .md h2, .md h3, .md h4, .md h5, .md h6 { color: #8B1538; }
.md a { color: #8B1538; }
.md a:hover { color: #A0446C; }

/* System fonts matching personal website */
body, .md { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, sans-serif; }
</style>

<!-- Markdeep: https://casual-effects.com/markdeep/ -->

# NeRF Fundamentals

**Module 5.1** | [‚Üê Back to Index](../index.html) | [Next: NeRF Acceleration ‚Üí](05-nerf-acceleration.md.html)

---

## Overview

Neural Radiance Fields (NeRF): Implicit 3D scene representation using MLPs. Represents scenes as continuous volumetric fields mapping 3D position and viewing direction to color and density.

**Key insight**: Instead of explicitly storing 3D geometry (meshes, point clouds), NeRF learns a continuous function that can be queried at any 3D point to predict what color a ray passing through that point would have when viewed from a specific direction.

[Figure placeholder: Side-by-side comparison showing explicit representation (mesh/point cloud) vs implicit NeRF representation, with visual illustration of querying NeRF at arbitrary 3D points]

**Why NeRF matters**: 
- **Novel view synthesis**: Generate views from arbitrary camera positions not seen during training
- **Implicit geometry**: No need for explicit mesh extraction (though it can be done via marching cubes)
- **High quality**: Can achieve photorealistic rendering quality
- **Compact representation**: Single MLP encodes entire scene

**Main limitations**:
- Slow training (hours to days)
- Slow rendering (seconds per frame)
- Requires many input views (typically 50-200+ images)
- Static scenes only (extensions handle dynamic scenes - see Module 10.1)

---

## Essential Papers

### üìñüî•üî• NeRF (2020)

**NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis** (Mildenhall et al.) | [arXiv](https://arxiv.org/abs/2003.08934) | [Project](https://www.matthewtancik.com/nerf)

**Key idea**: Represent scene as continuous function $F_\Theta: (x, y, z, \theta, \phi) \rightarrow (c, \sigma)$.

**Input**: 3D position $(x,y,z)$ and viewing direction $(\theta, \phi)$

**Output**: Color $c = (r,g,b)$ and volume density $\sigma$

**Architecture**:
1. Positional encoding of 3D coordinates and directions
2. MLP maps encoded position to density and feature vector
3. Additional MLP maps feature + viewing direction to color
4. Volume rendering integrates along camera rays

[Figure placeholder: NeRF architecture diagram showing:
- Input: 3D position (x,y,z) + viewing direction (Œ∏,œÜ)
- Positional encoding layers (sinusoidal encoding)
- Main MLP network (8 fully connected layers)
- Density output branch (single value œÉ)
- Feature vector output (256-dim)
- View-dependent color MLP (smaller MLP)
- Output: RGB color (r,g,b) + density œÉ
- Volume rendering integration along ray
- Final rendered pixel color]

**Training**: Minimize photometric loss between rendered and observed images.

**Requires**: Multiple posed camera views of static scene (typically from COLMAP).

**Typical setup**:
- 50-200 input images
- COLMAP for camera pose estimation
- Known camera intrinsics (focal length, principal point)
- Training takes 1-2 days on single GPU
- Output: MLP weights (~5-10MB for single scene)

[Figure placeholder: Training setup visualization showing:
- Input images arranged in 3D space with camera positions
- Camera frustums showing viewing directions
- Example rays being cast through scene
- Corresponding pixels in training images highlighted]

---

### üî• NeRF++ (2020)

**NeRF++: Analyzing and Improving Neural Radiance Fields** (Zhang et al.) | [arXiv](https://arxiv.org/abs/2010.07492)

**Improvements**:
- Handles unbounded scenes (inverted sphere parameterization)
- Better background modeling
- Improved training stability

---

### üî• Mip-NeRF (2021)

**Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields** (Barron et al.) | [arXiv](https://arxiv.org/abs/2103.13415)

**Key innovation**: Integrated positional encoding (IPE) - encodes conical frustums instead of points.

**Benefits**: Anti-aliasing, handles multi-scale scenes better, more efficient.

---

## Core Concepts

### Positional Encoding

**Purpose**: Help MLP represent high-frequency details (geometry, texture).

**The problem**: MLPs naturally learn low-frequency functions (smooth, blurry). Without encoding, NeRF struggles with sharp edges, fine details, and high-frequency textures.

**The solution**: Encode coordinates using sinusoidal functions at multiple frequencies. This allows the MLP to easily learn high-frequency patterns by mapping from encoded space rather than raw coordinates.

**Formulation**: 
$$\gamma(p) = (\sin(2^0 \pi p), \cos(2^0 \pi p), \sin(2^1 \pi p), \cos(2^1 \pi p), ..., \sin(2^{L-1} \pi p), \cos(2^{L-1} \pi p))$$

For 3D position: Encode $x, y, z$ coordinates independently with $L=10$ ‚Üí each coordinate becomes 20-dimensional vector ‚Üí total position encoding: 60 dimensions.

For viewing direction: Encode $(\theta, \phi)$ or normalized direction vector with $L=4$ ‚Üí 24-dimensional vector.

**Effect**: Enables learning fine details despite MLP's bias toward low frequencies.

[Figure placeholder: Visual comparison showing:
- NeRF without positional encoding: blurry, low-frequency output
- NeRF with positional encoding: sharp, detailed output
- Frequency spectrum visualization showing how encoding captures high frequencies]

**Why it works**: The encoding creates a mapping from continuous coordinates to a higher-dimensional space where different frequencies are separated into different dimensions. The MLP can then easily learn to attend to specific frequencies.

**Example**: 
- Input: $x = 0.5$
- Encoded: $[\sin(\pi \cdot 0.5), \cos(\pi \cdot 0.5), \sin(2\pi \cdot 0.5), \cos(2\pi \cdot 0.5), ...]$ 
- This encoding makes it easy for the MLP to detect if the coordinate is near $x=0.5$ (specific frequency patterns in the encoded space)

---

### Volume Rendering

**Ray marching**: Sample points along camera ray, query NeRF at each point, integrate.

**Rendering equation**:
$$C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \sigma(\mathbf{r}(t)) \mathbf{c}(\mathbf{r}(t), \mathbf{d}) dt$$

Where:
- $T(t) = \exp(-\int_{t_n}^{t} \sigma(\mathbf{r}(s)) ds)$: Transmittance (how much light reaches point)
- $\sigma(\mathbf{r}(t))$: Density at point
- $\mathbf{c}(\mathbf{r}(t), \mathbf{d})$: Color (view-dependent)

**Discretized**:
$$C(\mathbf{r}) = \sum_{i=1}^{N} T_i (1 - \exp(-\sigma_i \delta_i)) \mathbf{c}_i$$

Where $T_i = \exp(-\sum_{j=1}^{i-1} \sigma_j \delta_j)$.

[Figure placeholder: Volume rendering diagram showing:
- Camera ray cast from pixel through 3D space
- Sample points along ray (e.g., 64-128 points)
- At each sample point: query NeRF ‚Üí get density œÉ and color c
- Transmittance T decreases as ray encounters density
- Alpha compositing: blend colors weighted by density and transmittance
- Final pixel color = weighted sum of all sample colors]

**Intuition**: 
- Density œÉ: How "solid" is the material at this point? (0 = empty space, high = opaque)
- Transmittance T: How much light from the background reaches this point? (decreases as ray encounters dense material)
- The color contribution at each point is weighted by: density √ó transmittance (how visible is this point?)

**Implementation details**:
- Sample points uniformly along ray (or use stratified/importance sampling)
- Query NeRF at each sample (batch for efficiency)
- Integrate using alpha compositing formula above
- Handle background: if ray doesn't hit anything, use background color

[Figure placeholder: Step-by-step visualization:
1. Ray cast from camera
2. Sample points along ray
3. Query NeRF ‚Üí get œÉ and c for each point
4. Compute transmittance T for each point
5. Weight each color by T(1-exp(-œÉŒ¥))
6. Sum to get final pixel color]

---

### Hierarchical Sampling

**The problem**: Uniformly sampling points along every ray is wasteful. Most points are in empty space. We want more samples where there's actual geometry (high density).

**The solution**: Two-stage hierarchical sampling with two NeRF networks.

**Two-stage sampling**:
1. **Coarse network**: 
   - Sample 64 points uniformly along ray
   - Predict density at each point
   - Use these densities to create importance sampling distribution

2. **Fine network**:
   - Sample 64 additional points using importance sampling (more points where coarse network predicts high density)
   - Query fine network at all 128 points (64 uniform + 64 importance)
   - Render using fine network predictions

[Figure placeholder: Hierarchical sampling visualization:
- Top: Uniform sampling along ray (equal spacing)
- Bottom: Coarse network density prediction (bar chart showing density at each sample)
- Right: Importance sampling - more samples where density is high
- Final: Fine network queried at combined sample locations]

**Benefits**: 
- Efficiency - more samples where geometry exists
- Better quality - fine network gets more samples at important locations
- Faster training - coarse network is smaller, quick to evaluate

**Implementation**: 
- Coarse network: Smaller MLP (same architecture but fewer parameters)
- Both networks trained jointly with combined loss
- Importance sampling uses inverse transform sampling based on coarse density distribution

---

---

## Technical Details

### Loss Function

**Photometric loss**: 
$$L = \sum_{\mathbf{r}} ||C_c(\mathbf{r}) - C(\mathbf{r})||_2^2 + ||C_f(\mathbf{r}) - C(\mathbf{r})||_2^2$$

- $C_c$: Coarse network prediction
- $C_f$: Fine network prediction  
- $C$: Ground truth color from training image

**Notes**:
- Both networks contribute to loss (supervised jointly)
- L2 loss per pixel, summed over all sampled rays
- Each iteration samples random rays from random training images
- No regularization terms needed (NeRF naturally smooth due to MLP structure)

**Training batch size**: Typically 1024-4096 rays per batch (depends on GPU memory).

---

### Training Process

1. **Input preparation**: COLMAP provides camera poses, intrinsics
2. **Ray generation**: For each training image, sample random rays
3. **Ray marching**: Sample points along each ray
4. **MLP forward**: Query NeRF at each sample point
5. **Volume rendering**: Integrate to get predicted color
6. **Loss computation**: Compare with ground truth
7. **Backpropagation**: Update MLP weights

**Training time**: 
- Simple scenes: 1-3 hours
- Complex scenes: 12-24 hours
- Very high resolution: 1-2 days

**Optimization**: 
- Adam optimizer
- Learning rate: 5e-4 (decays during training)
- No batch normalization or dropout needed

**Convergence**: Typically 200,000-500,000 iterations (depends on scene).

[Figure placeholder: Training curve visualization showing:
- X-axis: Training iterations
- Y-axis: Loss value
- Two curves: Coarse network loss (decreasing) and Fine network loss (decreasing)
- Note showing convergence point]

---

## Problems Solved by NeRF

### Novel View Synthesis
**Problem**: Traditional 3D reconstruction produces explicit geometry (meshes/point clouds) that often have holes, artifacts, and struggle with view-dependent effects like reflections and transparency.

**NeRF solution**: Implicit representation learns view-dependent appearance naturally, can render views from arbitrary positions, handles specular surfaces and reflections well.

[Figure placeholder: Comparison showing mesh-based rendering (holes, artifacts) vs NeRF rendering (smooth, complete)]

### Photorealistic Quality
**Problem**: Previous neural rendering methods struggled to achieve photorealistic quality or required dense input views.

**NeRF solution**: Achieves state-of-the-art quality with relatively sparse input (50-200 views vs thousands needed for some methods).

### Compact Representation
**Problem**: Explicit 3D representations (meshes, point clouds, voxel grids) require large storage, especially for high-resolution scenes.

**NeRF solution**: Single MLP (~5-10MB) encodes entire scene, including geometry and appearance.

### View-Dependent Effects
**Problem**: Traditional graphics struggle with view-dependent appearance (specular highlights, reflections change with viewing angle).

**NeRF solution**: Viewing direction is explicit input, naturally models view-dependent effects.

---

## Remaining Challenges and Limitations

### Training and Inference Speed
**Problem**: 
- Training takes hours to days
- Rendering takes seconds per frame
- Not suitable for real-time applications

**Current solutions**: Acceleration methods (see Module 5.2: Instant-NGP, etc.) reduce training to minutes and enable real-time rendering, but with trade-offs.

**Open question**: Can we achieve both highest quality AND real-time? Gaussian Splatting (Module 6.1) offers alternative.

### Static Scenes Only
**Problem**: Original NeRF only handles static scenes. Real-world scenes have moving objects, people, dynamic lighting.

**Current solutions**: Dynamic NeRFs (Module 10.1) extend to dynamic scenes, but more complex and slower.

**Open question**: Can we handle arbitrary dynamic scenes with same quality and efficiency?

### Generalization
**Problem**: NeRF is scene-specific - each scene requires training from scratch. Cannot generalize across scenes or objects.

**Current solutions**: Some work on conditional NeRFs, but quality limited compared to scene-specific training.

**Open question**: Can we build generalizable NeRF models that work across scenes/objects without per-scene training?

### Few-Shot and Single-View
**Problem**: NeRF requires many input views (50-200+). Often impractical to capture that many images.

**Current solutions**: Some methods work with 3-5 views, but quality significantly degrades. 2D‚Üí3D methods (Module 7.2) address single-view but use different approaches.

**Open question**: Can we achieve NeRF-quality with just 1-3 views?

### Geometry Extraction
**Problem**: NeRF is implicit - extracting explicit geometry (meshes) requires marching cubes, can have artifacts.

**Open question**: Can we get clean, watertight meshes directly from NeRF without post-processing?

### Unbounded Scenes
**Problem**: Original NeRF struggles with large outdoor scenes, infinite backgrounds.

**Current solutions**: NeRF++ uses inverted sphere parameterization. MipNeRF-360 handles unbounded scenes.

**Remaining**: Still challenging for very large scenes, requires careful parameterization.

### Memory
**Problem**: High-resolution scenes or very detailed geometry can require large MLPs or many parameters.

**Current solutions**: Compression methods, quantization, smaller architectures with better encodings.

---

## Broader Insights and Implications

### Implicit vs Explicit Representations
**Insight**: NeRF demonstrates power of implicit representations. Rather than storing explicit geometry, learn continuous function. This paradigm shift enables new capabilities:
- Handling uncertainty naturally (what's behind an object? NeRF can interpolate)
- View-dependent effects come "for free"
- Compact storage for complex scenes

**Broader impact**: This implicit representation philosophy influences many subsequent works (Gaussian Splatting, Neural Fields, etc.)

### Differentiable Rendering Revolution
**Insight**: NeRF's use of differentiable volume rendering enables end-to-end learning from images. No need for intermediate explicit 3D representations.

**Broader impact**: Demonstrates that with proper differentiable rendering, we can learn 3D from 2D supervision alone. Influences work in generative 3D (Module 7), inverse rendering (Module 11.2).

### Quality vs Efficiency Trade-off
**Insight**: NeRF establishes that high quality is achievable, but at cost of efficiency. This creates clear research directions:
- Acceleration methods (Module 5.2)
- Alternative representations for speed (Gaussian Splatting)
- Compression for deployment

**Broader impact**: Highlights that representation choice fundamentally affects quality/efficiency trade-offs. Different methods may be optimal for different applications.

### The Role of Positional Encoding
**Insight**: Positional encoding is critical - it enables MLPs to represent high-frequency details. Without it, NeRF produces blurry results.

**Broader impact**: Positional encoding becomes standard in many neural field methods. Understanding its importance helps design better representations.

### Multi-View Geometry Meets Deep Learning
**Insight**: NeRF successfully combines classical multi-view geometry (COLMAP for poses) with deep learning (MLP for scene representation).

**Broader impact**: Shows value of combining classical CV techniques with modern deep learning. Not everything needs to be learned end-to-end.

[Placeholder for manual expansion: Add your own insights about NeRF's impact on the field, connections to other areas, philosophical implications]

---

## Implementation Example

**Basic NeRF architecture** (simplified pseudocode):

```python
# Positional encoding
def positional_encode(x, L=10):
    encodings = []
    for i in range(L):
        encodings.append(torch.sin(2**i * œÄ * x))
        encodings.append(torch.cos(2**i * œÄ * x))
    return torch.cat(encodings, dim=-1)

# NeRF forward pass
def nerf_forward(xyz, view_dir):
    # Encode inputs
    xyz_encoded = positional_encode(xyz, L=10)  # 60 dims
    view_encoded = positional_encode(view_dir, L=4)  # 24 dims
    
    # Main MLP
    h = xyz_encoded
    for layer in mlp_layers:
        h = relu(layer(h))
    
    # Output branches
    density = density_head(h)  # œÉ
    feature = feature_head(h)  # 256-dim
    
    # View-dependent color
    color_input = torch.cat([feature, view_encoded], dim=-1)
    color = color_head(color_input)  # RGB
    
    return color, density

# Volume rendering
def render_ray(ray_origin, ray_dir, nerf_model):
    # Sample points along ray
    t_samples = sample_points(near, far, N=64)
    points = ray_origin + t_samples * ray_dir
    
    # Query NeRF
    colors, densities = nerf_forward(points, ray_dir)
    
    # Integrate
    pixel_color = integrate(colors, densities, t_samples)
    return pixel_color
```

[Placeholder for manual expansion: Add complete implementation code, training loop, optimization details]

---

## Related Modules

- Module 3.2: Structure-from-Motion (provides camera poses)
- Module 5.2: NeRF Acceleration (speeds up training/inference)
- Module 6.1: Gaussian Splatting (alternative real-time approach)

---

## Advanced Topics

[Placeholder for manual expansion]:
- Explicit vs implicit geometry extraction (marching cubes from NeRF)
- View-dependent vs view-independent appearance
- Handling reflections and transparent objects
- Multi-scale NeRF representations
- Compression and quantization techniques
- Real-time NeRF (see Module 5.2: Acceleration)

---

## Benchmarks and Datasets

**Standard NeRF datasets**:
- **LLFF**: Real forward-facing scenes (8-15 images)
- **Synthetic NeRF**: 360¬∞ synthetic scenes with ground truth
- **MipNeRF-360**: Unbounded 360¬∞ scenes
- **Tanks and Temples**: Large outdoor scenes

**Metrics**:
- PSNR (Peak Signal-to-Noise Ratio): Typical 25-35 dB for good NeRF
- SSIM (Structural Similarity): Typical 0.9+ for good NeRF
- LPIPS (Learned Perceptual Image Patch Similarity): Lower is better

[Placeholder for manual expansion: Add comparison tables, benchmark results, dataset details]

---

## Additional Resources

- **NeRF Project Page**: [matthewtancik.com/nerf](https://www.matthewtancik.com/nerf)
- **NeRF Papers**: Comprehensive list of NeRF variants
- **Nerfstudio**: [nerf.studio](https://nerf.studio/) - NeRF framework
- **NeRF Implementation Tutorials**: [Placeholder - add links to tutorials]

---

<div style="text-align: center; margin-top: 2em;">
[‚Üê Back to Index](../index.html) | [Next: NeRF Acceleration ‚Üí](05-nerf-acceleration.md.html)
</div>

<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="markdeep.min.js" charset="utf-8"></script><script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js" charset="utf-8"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
