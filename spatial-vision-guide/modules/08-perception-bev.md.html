<!doctype html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>BEV Models</title>
<script src="https://casual-effects.com/markdeep/latest/markdeep.min.js"></script>
<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style>
</head>
<body>

<code style="display:none">
<!-- Markdeep: https://casual-effects.com/markdeep/ -->

# BEV Models

**Module 8.2** | [‚Üê Back to Index](../index.md.html) | [Next: Monocular 3D Detection ‚Üí](08-perception-monocular.md.html)

---

## Overview

Bird's-Eye-View (BEV) models: Transform multi-view camera features into unified BEV representation for 3D object detection. Common in autonomous driving.

---

## Essential Papers

### üî•üî• Lift-Splat-Shoot (2020)

**Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D** (Philion & Fidler) | [arXiv](https://arxiv.org/abs/2008.05711)

**Key idea**: Lift 2D features to 3D, splat to BEV grid, detect objects.

**Pipeline**:
1. **Lift**: For each pixel, predict depth distribution, create frustum of points
2. **Splat**: Project frustums to BEV grid, aggregate features
3. **Shoot**: 3D object detection in BEV space

[Figure placeholder: LSS pipeline diagram showing lift-splat-shoot process]

**Depth prediction**: Dense depth distribution per pixel (not single depth).

**BEV grid**: Regular 2D grid in bird's-eye view (e.g., 200m √ó 200m, 0.5m resolution).

**Advantages**: Handles arbitrary camera rigs, end-to-end trainable.

---

### üî• BEVDet (2021)

**BEVDet: High-Performance Multi-Camera 3D Object Detection** (Huang et al.) | [arXiv](https://arxiv.org/abs/2112.11790)

**Improvements over LSS**:
- Better backbone networks
- Data augmentation strategies
- Temporal fusion (BEVDet4D)

**Performance**: Strong results on nuScenes benchmark.

---

### üî•üî• BEVFormer (2022)

**BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers** (Li et al.) | [arXiv](https://arxiv.org/abs/2203.17270)

**Key innovation**: Transformer-based BEV feature learning with temporal fusion.

**Spatial cross-attention**: BEV queries attend to multi-view image features.

**Temporal self-attention**: Current BEV features attend to previous frame BEV features.

**BEV queries**: Learnable queries in BEV space, similar to DETR.

[Figure placeholder: BEVFormer architecture showing spatial and temporal attention]

**Advantages**:
- Better feature aggregation than LSS
- Temporal consistency
- Strong performance

---

### üî• BEVFusion (2022)

**BEVFusion: Multi-Task Multi-Sensor Fusion** (Liu et al.) | [arXiv](https://arxiv.org/abs/2205.13542)

**Multi-sensor fusion**: Fuses camera and LiDAR in BEV space.

**Multi-task**: Detection, segmentation, prediction.

**Unified BEV representation**: Single representation for all sensors and tasks.

---

## Core Concepts

### BEV Representation

**Bird's-Eye View**: Top-down 2D grid representation of 3D space.

**Grid structure**: Regular grid (e.g., 400√ó400 cells, each 0.5m√ó0.5m).

**Feature map**: Each cell contains aggregated features from multiple camera views.

**Advantages**: 
- Unified representation for multi-view
- Natural for autonomous driving (planning, prediction)
- Works well with LiDAR fusion

---

### Lift Operation

**Goal**: Lift 2D image features to 3D space.

**Approaches**:
- **LSS**: Depth distribution ‚Üí frustum of 3D points
- **Explicit depth**: Predict depth map, unproject to 3D
- **Implicit**: Learn 3D features directly

**Depth ambiguity**: Single view can't determine depth, multiple views needed.

---

### Splat Operation

**Goal**: Aggregate 3D features into BEV grid.

**Methods**:
- **Voxel pooling**: Aggregate points in each voxel
- **Nearest neighbor**: Assign to nearest BEV cell
- **Bilinear interpolation**: Smooth assignment

---

### Temporal Fusion

**Challenge**: Single frame lacks temporal context.

**Solutions**:
- **BEVFormer**: Attention to previous frame BEV features
- **BEVDet4D**: Concatenate multiple frames
- **Motion compensation**: Align features across frames

---

## Comparison with Multi-View Transformers

**BEV methods**: 
- 2D features ‚Üí BEV features ‚Üí 3D detection
- Explicit intermediate representation
- Good for planning/prediction tasks

**Multi-view transformers** (DETR3D, PETR):
- 2D features ‚Üí Direct 3D detection
- No explicit BEV intermediate
- Simpler architecture

**Trade-offs**: BEV methods often better for autonomous driving (planning, prediction). Multi-view transformers simpler and faster.

---

## Implementation Details

**Input**: Multiple camera images (e.g., 6 cameras around vehicle).

**Backbone**: ResNet or Vision Transformer to extract features.

**View transformation**: Lift-splat or transformer attention.

**Detection head**: 3D object detection (center, size, rotation, class).

**Loss**: Classification + regression losses.

---

## Datasets

**nuScenes**: 6 cameras, 3D annotations, common benchmark.

**Waymo**: 5 cameras, high resolution, more challenging.

---

## Related Modules

- Module 8.1: Multi-View Transformers (alternative approach)
- Module 3.1: Geometry & Camera Models (camera calibration)
- Module 4.1: Optical Flow (temporal features)

---

## Additional Resources

- **OpenMMLab Detection3D**: Implementations of BEV methods
- **nuScenes Dataset**: [nuscenes.org](https://www.nuscenes.org/)

---

<div style="text-align: center; margin-top: 2em;">
[‚Üê Back to Index](../index.md.html) | [Next: Monocular 3D Detection ‚Üí](08-perception-monocular.md.html)
</div>

</code>
</body>
</html>
