<!DOCTYPE html>
<meta charset="utf-8" emacsmode="-*- markdown -*-">
<link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/journal.css?">
<style>
/* Burgundy color theme override */
.md h1, .md h2, .md h3, .md h4, .md h5, .md h6 { color: #8B1538; }
.md a { color: #8B1538; }
.md a:hover { color: #A0446C; }

/* System fonts matching personal website */
body, .md { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, sans-serif; }
</style>

<!-- Markdeep: https://casual-effects.com/markdeep/ -->

# Score Distillation Sampling

**Module 7.1** | [‚Üê Back to Index](../index.html) | [Next: 2D‚Üí3D Lifting ‚Üí](07-sds-2d-to-3d.md.html)

---

## Overview

Score Distillation Sampling (SDS): Text-to-3D generation by optimizing 3D representations using pre-trained 2D diffusion models. Enables 3D generation without 3D training data.

---

## Essential Papers

### üî•üî•üî• DreamFusion (2022)

**DreamFusion: Text-to-3D using 2D Diffusion** (Poole et al.) | [arXiv](https://arxiv.org/abs/2209.14988) | [Project](https://dreamfusion3d.github.io/)

**Key idea**: Use 2D diffusion model as "loss function" for 3D optimization.

**Score Distillation Sampling (SDS)**:
$$\nabla_\theta L_{SDS} = \mathbb{E}_{t,\epsilon} [w(t)(\epsilon_\phi(x_t; y, t) - \epsilon) \frac{\partial x}{\partial \theta}]$$

Where:
- $\epsilon_\phi$: Pre-trained 2D diffusion model (e.g., Imagen)
- $x_t$: Rendered 2D view of 3D model at noise level $t$
- $y$: Text prompt
- $\theta$: 3D representation parameters
- $w(t)$: Time-dependent weighting

**Process**:
1. Initialize 3D representation (NeRF or mesh)
2. Render random viewpoint
3. Add noise to rendered image
4. Denoise with diffusion model
5. Compute gradient through rendering
6. Update 3D parameters

[Figure placeholder: SDS pipeline diagram showing 3D‚Üí2D‚Üídiffusion‚Üígradient flow]

**3D representation**: NeRF with instant-NGP encoding.

**Challenges**:
- Mode seeking (saturated colors)
- Slow optimization
- Geometry/appearance coupling

---

### üî• Magic3D (2023)

**Magic3D: High-Resolution Text-to-3D Content Creation** (Lin et al.) | [arXiv](https://arxiv.org/abs/2211.10440)

**Two-stage approach**:
1. **Coarse stage**: Low-resolution NeRF optimization with SDS
2. **Fine stage**: Extract mesh, refine with SDS at high resolution

**Improvements**:
- Higher resolution output
- Mesh-based fine-tuning
- Better geometry quality

---

### üî• Fantasia3D (2023)

**Fantasia3D: Disentangling Geometry and Appearance** (Chen et al.) | [arXiv](https://arxiv.org/abs/2303.13818)

**Key innovation**: Separate geometry and appearance optimization.

**Geometry**: Signed distance field (SDF) with normal-based rendering.

**Appearance**: Material properties (diffuse, specular, normal maps).

**Benefits**: Better geometry quality, realistic materials, controllable appearance.

---

### üî• ProlificDreamer (2023)

**ProlificDreamer: High-Fidelity Text-to-3D** (Wang et al.) | [arXiv](https://arxiv.org/abs/2305.16213)

**Variational Score Distillation (VSD)**: Addresses mode-seeking issues in SDS.

**Key idea**: Learn distribution over 3D objects, not single point estimate.

**Benefits**: Higher diversity, better quality, more stable optimization.

---

## Core Concepts

### Score Distillation Sampling

**Intuition**: Diffusion model knows what "good" images look like. Use its gradients to guide 3D optimization.

**Formulation**: Treat diffusion loss as differentiable loss function for 3D.

**Advantage**: Leverages powerful pre-trained 2D models without 3D data.

**Limitation**: No 3D consistency guarantee (each view optimized independently).

---

### Variational Score Distillation

**Problem**: SDS mode-seeking behavior.

**Solution**: Learn distribution $q_\theta(z)$ over 3D representations.

**Objective**: Minimize KL divergence between rendered distribution and diffusion prior.

**Benefits**: Better diversity, smoother optimization.

---

### Geometry vs Appearance Disentanglement

**Problem**: SDS couples geometry and appearance.

**Solution**: Separate optimization:
- **Geometry**: Use normal-based rendering (geometry-only)
- **Appearance**: Material properties on fixed geometry

**Benefits**: Better control, higher quality geometry.

---

## Technical Details

### SDS Implementation

**Rendering**: Differentiable renderer (NeRF, mesh, or Gaussian splatting).

**Diffusion model**: Pre-trained text-to-image model (Imagen, Stable Diffusion).

**Optimization**: Adam or similar, gradient through rendering.

**Noise schedule**: Sample noise level $t \sim \mathcal{U}(0.02, 0.98)$.

**View sampling**: Random camera poses, importance sample based on prompt.

---

---

## Problems Solved by Score Distillation Sampling

### Text-to-3D Without 3D Data
**Problem**: Training 3D generative models requires large 3D datasets, which are scarce and expensive. Text-to-3D generation was challenging.

**SDS solution**: Leverages pre-trained 2D text-to-image diffusion models. No need for 3D training data - generate 3D from 2D supervision via score distillation.

[Figure placeholder: Visualization showing how 2D diffusion model supervises 3D generation through rendering and score distillation]

### Using Powerful 2D Priors for 3D
**Problem**: 2D image models (e.g., Stable Diffusion) have learned rich priors about appearance, but 3D models needed to learn from scratch.

**SDS solution**: Directly leverages 2D priors through score distillation. Transfers knowledge from 2D to 3D without retraining.

### Flexible 3D Representations
**Problem**: Different 3D representations (NeRF, meshes, Gaussians) each require different generation approaches.

**SDS solution**: Works with any differentiable 3D representation. Same distillation process applies to NeRF, meshes, Gaussians, etc.

### Single Unified Framework
**Problem**: Different text-to-3D tasks (object generation, scene generation, editing) needed different methods.

**SDS solution**: Unified framework - change the 3D representation or prompt, same distillation process. Applies to objects, scenes, editing.

---

## Remaining Challenges and Limitations

### Mode Seeking / Janus Problem
**Problem**: SDS can produce multi-faced objects (Janus problem) or collapse to degenerate solutions due to mode-seeking behavior.

**Current solutions**: VSD (Variational Score Distillation), classifier-free guidance scheduling, but problem not fully solved.

**Open question**: Better understanding of mode-seeking? More robust optimization strategies?

### Slow Convergence
**Problem**: SDS requires many iterations (thousands to tens of thousands) to converge, making it computationally expensive.

**Current solutions**: Multi-resolution optimization, better initialization, but still slow.

**Open question**: Faster convergence without quality loss? Better optimization algorithms?

### Geometry Quality
**Problem**: Generated geometry can have artifacts, holes, or poor topology. Not always watertight or clean.

**Current solutions**: SDF-based methods improve geometry, normal constraints help, but quality still variable.

**Open question**: Guaranteed clean geometry? Better geometric priors?

### View Consistency
**Problem**: Views from different angles may be inconsistent - same object can look different from different viewpoints.

**Current solutions**: Multi-view consistency losses, regularization terms, but consistency not perfect.

**Open question**: Guaranteed multi-view consistency? Better consistency enforcement?

### Limited Control
**Problem**: While text provides control, precise spatial control (e.g., place object at specific location in scene) is limited.

**Current solutions**: ControlNet-style conditioning, but still limited compared to explicit control.

**Open question**: Fine-grained spatial and compositional control?

### Scaling to Complex Scenes
**Problem**: Works well for single objects, but complex scenes with multiple objects, relationships, lighting remain challenging.

**Open question**: Better scene composition? Handling object relationships? Complex lighting?

### Quality vs Efficiency Trade-off
**Problem**: Higher quality requires more iterations, better initialization, larger models. Trade-off between quality and speed.

**Current solutions**: Various optimizations, but fundamental trade-off remains.

**Open question**: Can we achieve highest quality with much less computation?

---

## Broader Insights and Implications

### 2D Supervision for 3D Generation
**Insight**: SDS demonstrates that 2D supervision (from powerful pre-trained models) can effectively train 3D representations. No need for 3D data when we have good 2D priors.

**Broader impact**: Enables leveraging vast amounts of 2D data for 3D tasks. Opens up 3D generation to benefit from 2D model advances without requiring 3D datasets.

### The Power of Pre-Trained Models
**Insight**: Score distillation leverages massive pre-trained diffusion models. This transfer learning paradigm crucial for quality.

**Broader impact**: Shows value of large-scale pre-training. As 2D models improve, 3D generation automatically benefits. Highlights importance of foundation models.

### Differentiable Rendering Enables Learning
**Insight**: Differentiable rendering (rendering 3D ‚Üí 2D images) enables backpropagation from 2D losses to 3D parameters.

**Broader impact**: Demonstrates power of differentiable rendering paradigm. Enables end-to-end learning from 2D supervision. Influences design of other 3D methods.

### Optimization-Based vs Feed-Forward Generation
**Insight**: SDS uses optimization (iterative refinement) rather than feed-forward generation. Each generation requires optimization process.

**Broader impact**: Shows that optimization-based generation can achieve high quality, even if slower. Trade-off between quality (optimization) and speed (feed-forward) is design choice.

### The Role of Score Functions
**Insight**: Score distillation uses score functions (gradients of log-probability) from diffusion models. This connects diffusion theory to 3D optimization.

**Broader impact**: Provides theoretical foundation connecting 2D diffusion to 3D generation. May inspire other score-based 3D methods.

[Placeholder for manual expansion: Add insights about democratizing 3D content creation, impact on industries, connections to other generative 3D methods, future directions]

---

## Related Methods

**SJC (Score Jacobian Chaining)**: Alternative formulation, similar to SDS.

**VSD**: Variational extension addressing mode-seeking.

**3D-aware diffusion**: Direct 3D diffusion (still limited).

---

**SJC (Score Jacobian Chaining)**: Alternative formulation, similar to SDS.

**VSD**: Variational extension addressing mode-seeking.

**3D-aware diffusion**: Direct 3D diffusion (still limited).

---

## Applications

- Text-to-3D asset generation
- 3D content creation
- Concept visualization
- Game/VR asset pipeline

---

## Related Modules

- Module 2.1: Diffusion Models (underlying 2D models)
- Module 5.1: NeRF Fundamentals (3D representation)
- Module 6.1: Gaussian Splatting (alternative 3D representation)
- Module 7.2: 2D‚Üí3D Lifting (single image input)

---

## Additional Resources

- **DreamFusion Project**: [dreamfusion3d.github.io](https://dreamfusion3d.github.io/)
- **Stable Diffusion**: Underlying 2D model used by many methods
- **Community implementations**: Various open-source SDS implementations

---

<div style="text-align: center; margin-top: 2em;">
[‚Üê Back to Index](../index.html) | [Next: 2D‚Üí3D Lifting ‚Üí](07-sds-2d-to-3d.md.html)
</div>

<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="markdeep.min.js" charset="utf-8"></script><script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js" charset="utf-8"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>

<script src="../assets/search.js"></script>
