<!doctype html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Core ML & Mathematics Foundations</title>
<script src="https://casual-effects.com/markdeep/latest/markdeep.min.js"></script>
<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style>
</head>
<body>

<code style="display:none">
<!-- Markdeep: https://casual-effects.com/markdeep/ -->

# Core ML & Mathematics Foundations

**Module 0.1** | [← Back to Index](../index.md.html) | [Next: Classical CV →](00-foundations-classical-cv.md.html)

---

## Overview

Essential machine learning theory, optimization, and mathematical tools for 3D/4D vision and neural rendering.

---

## Required Reading

## Resources

### Deep Learning Book (Goodfellow, Bengio, Courville)
[deeplearningbook.org](https://www.deeplearningbook.org/)

**Key chapters:** 5 (ML Basics), 6 (Feedforward Networks), 7 (Regularization), 8 (Optimization), 9 (CNNs), 14 (Autoencoders), 20 (Generative Models)

---

### Dive Into Deep Learning (Zhang et al.)
[d2l.ai](https://d2l.ai/)

Practical implementations. Focus: CNNs, attention mechanisms, computer vision sections.

---

### Pattern Recognition and Machine Learning (Bishop)
[Free PDF](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)

**Key chapters:** 1 (Introduction), 2 (Probability), 3 (Linear Models), 5 (Neural Networks)

---

### CS231n: Convolutional Neural Networks for Visual Recognition
[cs231n.stanford.edu](https://cs231n.github.io/)

**Topics:** CNNs, optimization (SGD, Adam), backpropagation, architectures (LeNet, AlexNet, VGG, ResNet)

---

### CS231A: Computer Vision, from 3D Reconstruction to Recognition
[Course materials](http://web.stanford.edu/class/cs231a/)

**Topics:** Camera models, epipolar geometry, Structure-from-Motion, multi-view geometry

---

## Additional Resources

- **Mathematics for Machine Learning** (Deisenroth, Faisal, Ong) - Linear algebra and calculus
- **Numerical Linear Algebra** (Trefethen & Bau) - Optimization foundations
- **Information Theory** basics - VAEs and generative models

---

## Key Concepts

### Optimization Methods

**SGD (Stochastic Gradient Descent)**: Standard gradient-based optimization.

**Adam/AdamW**: Adaptive learning rate optimization, widely used in practice.

**Learning rate schedules**: Step decay, cosine annealing, warmup.

**Gradient clipping**: Prevents exploding gradients in RNNs/transformers.

Used in: NeRF training, score distillation, differentiable rendering.

---

### Backpropagation

**Forward pass**: Compute predictions through network.

**Backward pass**: Compute gradients via chain rule.

**Computational graphs**: Track gradient flow through operations.

---

### Convolutional Neural Networks

**Convolution**: Local feature extraction via sliding filters.

**Pooling**: Spatial downsampling (max, average).

**Architectures**: ResNet, EfficientNet, Vision Transformers.

CNNs are backbones for 3D vision models (feature extractors, encoders).

---

## Why These Foundations Matter for 3D Vision

### Optimization Enables Learning
**Insight**: Gradient-based optimization (SGD, Adam) is foundation of all neural learning. Without it, cannot train neural networks for 3D tasks.

**Broader impact**: Optimization methods directly affect training quality and convergence. Important to understand for effective training.

### Differentiable Operations Enable 3D Learning
**Insight**: Differentiable rendering, differentiable operations enable learning 3D from 2D supervision. Backpropagation through rendering enables NeRF, Gaussian Splatting, etc.

**Broader impact**: Differentiability is key enabling factor for modern 3D vision. Makes previously impossible learning tasks possible.

### Feature Extraction for 3D Understanding
**Insight**: CNNs provide powerful feature extractors used throughout 3D vision (feature matching, BEV detection, multi-view fusion). Foundation for representation learning.

**Broader impact**: CNN architectures influence design of 3D vision systems. Better features → better 3D understanding.

---

## Remaining Challenges in ML Foundations

### Optimization Challenges
**Problem**: Non-convex optimization, local minima, hyperparameter tuning remain challenging.

**Open question**: Better optimization methods? Automatic hyperparameter tuning?

### Computational Resources
**Problem**: Training large models requires significant computational resources.

**Current solutions**: Distributed training, efficient architectures, but still resource-intensive.

### Generalization
**Problem**: Models may not generalize well to new domains or data distributions.

**Open question**: Better generalization? Domain adaptation methods?

---

## Broader Insights and Implications

### The Role of Mathematics
**Insight**: Strong mathematical foundations (linear algebra, calculus, optimization) essential for understanding and improving deep learning methods.

**Broader impact**: Mathematics provides tools to understand, analyze, and improve methods. Not just implementation - need mathematical understanding.

### Theory vs Practice
**Insight**: While theory provides guidance, practice often requires empirical tuning and experimentation. Both theory and practice important.

**Broader impact**: Demonstrates value of both theoretical understanding and practical experience. Best researchers combine both.

[Placeholder for manual expansion: Add insights about learning, teaching, connections to 3D vision]

---

## Related Modules

- Module 0.3: Transformers
- Module 3.1: Camera models (linear algebra)
- Module 5.1: NeRF training (optimization)
- Module 7.1: Score Distillation

---

## Additional Resources

- 3Blue1Brown: Neural Network series
- Fast.ai: Practical deep learning
- PyTorch Tutorials
- Papers with Code

---

---

<div style="text-align: center; margin-top: 2em;">
[← Back to Index](../index.md.html) | [Next: Classical CV →](00-foundations-classical-cv.md.html)
</div>

</code>
</body>
</html>
