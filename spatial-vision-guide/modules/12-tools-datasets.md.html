<!doctype html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Datasets</title>
<script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js"></script>
<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style>
</head>
<body>

<code style="display:none">
<!-- Markdeep: https://casual-effects.com/markdeep/ -->

# Datasets

**Module 12.2** | [← Back to Index](../index.md.html) | [Previous: Libraries & Tools ←](12-tools-libraries.md.html)

---

## Overview

Key datasets for 3D/4D computer vision, neural rendering, and spatial understanding. From autonomous driving to indoor scenes.

---

## Autonomous Driving

### nuScenes
**Website**: [nuscenes.org](https://www.nuscenes.org/)

**Purpose**: Multi-modal autonomous driving dataset.

**Features**:
- 6 cameras (360° coverage)
- LiDAR
- Radar
- 3D bounding boxes
- Maps

**Use cases**: BEV detection, multi-view perception, tracking

---

### Waymo Open Dataset
**Website**: [waymo.com/open](https://waymo.com/open)

**Purpose**: Large-scale autonomous driving dataset.

**Features**:
- 5 cameras
- LiDAR
- High resolution
- Dense annotations

**Use cases**: 3D detection, BEV models, perception

---

### KITTI
**Website**: [www.cvlibs.net/datasets/kitti](http://www.cvlibs.net/datasets/kitti)

**Purpose**: Classic autonomous driving benchmark.

**Features**:
- Stereo cameras
- LiDAR
- 2D/3D object detection
- Tracking

**Use cases**: Monocular 3D detection, tracking

---

## Indoor Scenes

### ScanNet
**Website**: [scannet.princeton.edu](http://scannet.princeton.edu/)

**Purpose**: Large-scale indoor scene dataset.

**Features**:
- RGB-D videos
- 3D meshes
- Semantic labels
- Camera poses

**Use cases**: Scene understanding, 3D reconstruction, SLAM

---

### 3D-FRONT / 3D-FUTURE
**Purpose**: Synthetic indoor scenes.

**Features**: High-quality 3D scenes, furniture models

**Use cases**: Scene generation, 3D understanding

---

## Neural Rendering

### NeRF Datasets
**Common datasets**:
- **LLFF**: Real forward-facing scenes
- **Synthetic NeRF**: Synthetic 360° scenes
- **MipNeRF-360**: 360° unbounded scenes
- **Tanks and Temples**: Large scenes

**Features**: Multi-view images, camera poses, often generated with COLMAP

---

### Dynamic NeRF Datasets
- **HyperNeRF**: Dynamic scenes
- **D-NeRF**: Synthetic dynamic scenes

---

## Human Datasets

### Human3.6M
**Purpose**: 3D human pose dataset.

**Features**: MoCap data, multiple views, actions

---

### 3DPW
**Purpose**: 3D Pose in the Wild.

**Features**: Outdoor scenes, SMPL fits

---

### THUMOS
**Purpose**: Human action recognition.

---

## Optical Flow & Matching

### Sintel
**Purpose**: Optical flow benchmark.

**Features**: Synthetic scenes, ground truth flow

---

### KITTI (see above)
Also includes optical flow ground truth.

---

### FlyingThings3D
**Purpose**: Scene flow dataset.

**Features**: Synthetic scenes, 3D motion

---

## Why Datasets Matter for 3D Vision

### Enabling Benchmarking and Progress
**Insight**: Standardized datasets enable fair comparison of methods and drive research progress. Without benchmarks, hard to evaluate improvements.

**Broader impact**: Datasets are infrastructure for research. Good datasets enable better research.

### Diversity and Generalization
**Insight**: Diverse datasets (different scenes, conditions, domains) enable training models that generalize well. Dataset diversity crucial for robustness.

**Broader impact**: Shows importance of data collection and curation. Better datasets → better models.

### Real-World vs Synthetic
**Insight**: Real-world datasets provide authentic challenges (noise, occlusions, lighting) but expensive. Synthetic datasets provide control and scale but may not transfer.

**Broader impact**: Trade-off between real-world authenticity and synthetic control. Often need both.

---

## Challenges in Dataset Creation

### Annotation Cost
**Problem**: 3D annotations (3D boxes, poses, meshes) expensive and time-consuming to create.

**Open question**: Better annotation tools? Semi-supervised or self-supervised methods?

### Scale and Diversity
**Problem**: Need large, diverse datasets for robust models. Collection and curation expensive.

**Current solutions**: Community efforts, synthetic data, but challenges remain.

### Evaluation Metrics
**Problem**: Different datasets may use different metrics. Hard to compare across datasets.

**Open question**: Standardized evaluation? Better metrics?

---

## Broader Insights and Implications

### Data as Research Infrastructure
**Insight**: Datasets are research infrastructure - they enable and constrain what research can be done. Investment in datasets crucial.

**Broader impact**: Shows that data infrastructure is as important as algorithmic research. Need both for progress.

### The Role of Benchmarks
**Insight**: Benchmarks drive research directions. Choice of benchmarks influences what problems get solved.

**Broader impact**: Demonstrates that benchmark design is important research contribution. Influences field direction.

[Placeholder for manual expansion: Add insights about open science, reproducibility, community]

---

## Related Modules

- Module 3.2: Structure-from-Motion (COLMAP data processing)
- Module 5.1: NeRF Fundamentals (NeRF datasets)
- Module 8.2: BEV Models (nuScenes, Waymo)

---

## Additional Resources

- **Papers with Code Datasets**: [paperswithcode.com/datasets](https://paperswithcode.com/datasets)
- Individual dataset websites for download and documentation

---

<div style="text-align: center; margin-top: 2em;">
[← Back to Index](../index.md.html) | [Previous: Libraries & Tools ←](12-tools-libraries.md.html)
</div>

</code>
</body>
</html>
