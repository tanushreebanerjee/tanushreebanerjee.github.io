<!doctype html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Multi-View Transformers for 3D Detection</title>
<script src="https://casual-effects.com/markdeep/latest/markdeep.min.js"></script>
<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style>
</head>
<body>

<code style="display:none">
<!-- Markdeep: https://casual-effects.com/markdeep/ -->

# Multi-View Transformers for 3D Detection

**Module 8.1** | [‚Üê Back to Index](../index.md.html) | [Next: BEV Models ‚Üí](08-perception-bev.md.html)

---

## Overview

Multi-view transformers for 3D object detection from multiple camera viewpoints. Transformer architectures reason about 3D space directly, unlike BEV methods that first lift to bird's-eye view.

Covers: **DETR3D, PETR, MV2D**, and related methods.

---

## Core Concepts

### The Multi-View Detection Problem

**Input**: Multiple camera images from different viewpoints (e.g., 6 cameras around vehicle)

**Output**: 3D object detections in shared 3D coordinate frame

**Why multi-view is harder than single-view**:
- **Multi-view fusion**: Must combine information from different viewpoints
- **3D reasoning from 2D features**: Need to recover 3D structure from 2D observations
- **Scale differences**: Objects appear at different scales across views
- **Occlusion**: Objects visible in some views but not others
- **Depth ambiguity**: Single view cannot determine depth; multiple views required

---

## Essential Papers

### üî•üî• DETR3D (2021)

**DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries**

- **Authors:** Wang et al. (Waymo Research)
- **arXiv:** [2110.06922](https://arxiv.org/abs/2110.06922)
- **Impact:** First transformer-based multi-view 3D detection

**Key idea:** Instead of 2D queries like DETR, use **3D object queries**. Project these 3D queries to 2D to sample features from image views, then refine 3D box predictions.

**Architecture:**
1. **3D Object Queries**: Learnable 3D reference points (unlike DETR's 2D queries)
2. **Cross-view attention**: Project queries to each camera view using camera calibration, sample features via bilinear interpolation
3. **3D box refinement**: Update query positions and box parameters iteratively through transformer decoder layers

**How DETR3D uses 3D queries**: Queries are 3D points in world space. Each query projects to 2D locations in each camera view (using intrinsics/extrinsics), samples features from those locations, then cross-attends across views to aggregate information and refine the 3D position.

**Key insight**: "3D-to-2D" approach: start with 3D hypotheses, use 2D features to validate/refine.

Established the query-based paradigm for multi-view 3D detection.

---

### üî•üî• PETR / PETRv2 / PETRv3 (2022-2023)

**PETR: Position Embedding Transformation for Multi-View 3D Object Detection**

- **Authors:** Liu et al.
- **arXiv:** 
  - PETR: [2203.05625](https://arxiv.org/abs/2203.05625)
  - PETRv2: [2206.01256](https://arxiv.org/abs/2206.01256)
  - PETRv3: (2023)

**Key idea:** Encode 3D position information directly into 2D image features, then use standard transformer attention (no explicit 3D queries or projections).

**PETR innovations:**
1. **3D Position Embedding**: Each 2D feature gets a 3D position embedding (via camera intrinsics/extrinsics)
2. **Unified representation**: Features contain both appearance (from images) and geometry (position embeddings)
3. **Standard transformer**: Use standard self-attention, no custom projection operations

**How PETR encodes 3D information**: Rather than maintaining explicit 3D queries, PETR embeds 3D position directly into 2D features. For each pixel, it unprojects to a 3D ray, samples points along the ray at different depths, encodes those 3D positions using positional encoding (similar to NeRF), then adds these embeddings to the image features. This allows standard transformers to reason about 3D space implicitly.

**PETRv2 improvements:**
- Temporal modeling for video sequences
- Better multi-scale features
- Improved performance on nuScenes

**PETRv3:**
- Further temporal refinement
- Streaming inference support

**Advantages**: Simpler than DETR3D (no iterative refinement), faster training. Explicit 3D queries unnecessary with proper position encoding.

---

### üî• MV2D (2023)

**MV2D: Multi-View 2D Object Detection**

- **Impact:** Approaches multi-view detection as 2D detection problem with geometric constraints
- **Key insight:** Sometimes simpler 2D approaches with geometric post-processing work well


---

### Related Methods

- **MVDeTr / MVDet**: Multi-view detection variants with different fusion strategies
- **StreamPETR**: Streaming/online inference for multi-view detection (2024)

---

## Architecture Comparison

### DETR3D Approach

```
Multi-view Images ‚Üí Image Encoder ‚Üí 2D Features
                                            ‚Üì
3D Object Queries ‚Üê‚Üí [Project to 2D] ‚Üê‚Üí Sample Features
      ‚Üì
Refine 3D Boxes
```

**Strengths:**
- Explicit 3D reasoning
- Iterative refinement
- Clear interpretation

**Weaknesses:**
- Requires camera calibration for projection
- Iterative refinement can be slow

---

### PETR Approach

```
Multi-view Images ‚Üí Image Encoder ‚Üí 2D Features
                         ‚Üì
              Add 3D Position Embeddings
                         ‚Üì
            Unified Feature Representation
                         ‚Üì
              Standard Transformer Attention
                         ‚Üì
                 3D Box Predictions
```

**Strengths:**
- Simpler architecture
- No explicit projection needed
- Faster training/inference

**Weaknesses:**
- Position embeddings must be carefully designed
- Less explicit 3D reasoning

### Trade-offs: DETR3D vs PETR

**DETR3D**: Explicit 3D reasoning with iterative refinement. Requires precise camera calibration. Better interpretability, but slower.

**PETR**: Implicit 3D reasoning via position embeddings. Simpler, faster, but less interpretable. Position embedding design is critical.

---

## Key Technical Details

### 3D Position Embeddings (PETR)

Each 2D feature location $(u, v)$ can be transformed to a 3D ray. The key is encoding this 3D information:

1. **Unproject to 3D ray**: Use camera intrinsics to get ray direction
2. **Sample points along ray**: Create 3D points at different depths
3. **Encode positions**: Use positional encoding (like NeRF) for 3D coordinates
4. **Add to features**: Concatenate or add to image features

**Mathematical formulation:**

For pixel $(u, v)$ in camera $c$:
- Unproject: $d \cdot K_c^{-1} \begin{bmatrix} u \\ v \\ 1 \end{bmatrix}$ gives 3D point
- Encode: $\text{PE}(x, y, z)$ using sinusoidal encoding
- Feature: $f = f_{\text{image}} + f_{\text{position}}$

---

### 3D Object Queries (DETR3D)

Learnable 3D reference points that get refined:

1. **Initialization**: Random or learned 3D positions
2. **Projection**: $P_{\text{2D}} = K \cdot [R|t] \cdot P_{\text{3D}}$
3. **Feature sampling**: Bilinear interpolation from 2D features
4. **Attention**: Cross-attend over views
5. **Update**: Refine 3D position and box parameters

---

## Connections to Other Methods

### Relation to DETR (2D Detection)

- **DETR**: 2D object queries ‚Üí 2D features ‚Üí 2D boxes
- **DETR3D**: 3D object queries ‚Üí (project to) 2D features ‚Üí 3D boxes

Same transformer architecture, different query space. DETR3D extends DETR's query-based detection to 3D by using 3D queries instead of 2D queries.

### Relation to NeRF

Both encode 3D position information into features:
- **NeRF**: Position encoding for 3D coordinates (sinusoidal encoding of (x,y,z) points)
- **PETR**: Position encoding for multi-view 3D reasoning (encodes 3D rays/points into 2D features)

The positional encoding technique from NeRF is adapted here to embed geometric information into multi-view detection features.

### Relation to BEV Methods

- **BEV**: 2D features ‚Üí BEV features ‚Üí 3D detection
- **Multi-view transformers**: 2D features ‚Üí (3D-aware) ‚Üí Direct 3D detection

Multi-view transformers skip the BEV intermediate representation, reasoning about 3D space directly from 2D features.

---

## Implementation Notes

### Key Components

1. **Image encoder**: ResNet, EfficientNet, or Vision Transformer backbone
2. **3D position encoding**: Camera calibration ‚Üí unproject ‚Üí positional encoding
3. **Transformer decoder**: Standard transformer layers (like DETR)
4. **3D box head**: Predict center, size, rotation, class

### Common Pitfalls

- **Camera calibration errors**: Multi-view methods are sensitive to calibration
- **Depth ambiguity**: Need multi-view fusion to resolve depth
- **Feature alignment**: Features from different views must align properly

---

## Datasets

### nuScenes

- 6 cameras around vehicle (360¬∞ coverage)
- 3D object annotations
- Common benchmark for multi-view detection

### Waymo Open Dataset

- 5 cameras
- High resolution
- More challenging scenarios

---

## Performance Trends

**Evolution:**
1. **DETR3D** (2021): Established transformer-based approach
2. **PETR** (2022): Simplified with position embeddings
3. **PETRv2** (2022): Added temporal modeling
4. **PETRv3** (2023): Further improvements
5. **Recent work**: Better efficiency, streaming inference

**Current state:** PETR-family methods are widely used in industry (autonomous driving).

---

## Reading Order

1. DETR3D - Explicit 3D reasoning
2. PETR - Position embeddings approach
3. PETRv2 - Temporal extensions
4. BEVFormer (next module) - Compare with BEV methods

---

## Additional Resources

- **OpenMMLab Detection3D**: [github.com/open-mmlab/mmdetection3d](https://github.com/open-mmlab/mmdetection3d) - Implementations
- **nuScenes Detection Challenge**: See leaderboard and methods
- **Camera Calibration Tutorial**: Essential prerequisite understanding

---

## Related Modules

- **Module 8.2**: BEV Models (alternative approach to multi-view fusion)
- **Module 8.3**: Monocular 3D Detection (single-view methods)
- **Module 3.1**: Geometry foundations

---

<div style="text-align: center; margin-top: 2em;">
[‚Üê Back to Index](../index.md.html) | [Next: BEV Models ‚Üí](08-perception-bev.md.html)
</div>

</code>
</body>
</html>
