<!doctype html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Transformers & Attention</title>
<script src="https://casual-effects.com/markdeep/latest/markdeep.min.js"></script>
<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style>
</head>
<body>

<code style="display:none">
<!-- Markdeep: https://casual-effects.com/markdeep/ -->

# Transformers & Attention

**Module 0.3** | [‚Üê Back to Index](../index.md.html) | [Next: Representation Learning ‚Üí](../index.md.html)

---

## Overview

Transformer architecture and attention mechanisms. Used in modern vision models, multi-view transformers, and BEV methods.

---

## Essential Papers

### üî• Attention Is All You Need (2017)

**Vaswani et al.** | [arXiv](https://arxiv.org/abs/1706.03762)

**Key idea:** Self-attention mechanism replaces recurrent layers for sequence modeling.

**Components:**
- **Multi-head attention**: Multiple attention heads capture different relationships
- **Self-attention**: $Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$
- **Positional encoding**: Adds position information to input embeddings
- **Feed-forward networks**: Point-wise MLPs
- **Layer normalization**: Normalizes activations

**Impact**: Foundation for modern NLP and vision transformers.

---

### üî• Vision Transformer (ViT) (2020)

**An Image is Worth 16x16 Words** (Dosovitskiy et al.) | [arXiv](https://arxiv.org/abs/2010.11929)

**Key idea:** Apply transformer architecture directly to image patches.

**Architecture:**
1. Split image into patches (e.g., 16√ó16)
2. Linear projection to embeddings
3. Add positional embeddings
4. Transformer encoder (self-attention + MLP)
5. Classification head

[Figure placeholder: ViT architecture diagram]

**Advantages**: Global receptive field, scales well with data.

**Limitations**: Requires large datasets (ImageNet-21k, JFT-300M) for good performance.

---

### üî• DeiT (2021)

**Training data-efficient image transformers** (Touvron et al.) | [arXiv](https://arxiv.org/abs/2012.12877)

**Key innovation**: Knowledge distillation from CNN teacher to train ViT on ImageNet.

**Components:**
- Distillation token (learns from teacher)
- Data augmentation strategies
- Efficient training without massive datasets

Enables ViT to work well on ImageNet-scale data.

---

### üî• Swin Transformer (2021)

**Swin Transformer: Hierarchical Vision Transformer using Shifted Windows** (Liu et al.) | [arXiv](https://arxiv.org/abs/2103.14030)

**Key idea:** Hierarchical transformer with shifted windows for efficient computation.

**Features:**
- **Window-based attention**: Limits self-attention to local windows
- **Shifted windows**: Connects windows across layers
- **Hierarchical structure**: Multi-scale feature maps (like CNNs)

**Advantages**: Linear complexity with respect to image size, works as backbone for detection/segmentation.

[Figure placeholder: Swin transformer shifted window pattern]

---

## Key Concepts

### Self-Attention Mechanism

**Query (Q), Key (K), Value (V)**: Three learned linear projections.

**Attention score**: Measures relevance between positions: $QK^T$

**Attention weights**: Softmax of scores, normalize to probabilities.

**Output**: Weighted sum of values based on attention weights.

**Interpretation**: Each position "attends" to relevant positions in the sequence.

---

### Multi-Head Attention

Multiple attention heads in parallel capture different types of relationships.

**Formula**: $MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O$ where each head is independent attention.

**Benefits**: Captures different relationships (e.g., local vs. global, different semantic concepts).

---

### Positional Encoding

**Problem**: Attention is permutation-invariant, needs position information.

**Solutions**:
- **Absolute positional encoding**: Sinusoidal functions of position indices
- **Learned positional embeddings**: Learned parameters
- **Relative positional encoding**: Encodes relative positions (RoPE)

---

### Layer Normalization

Normalizes activations across features for each sample.

**Formula**: $LN(x) = \gamma \frac{x - \mu}{\sigma} + \beta$ where $\mu, \sigma$ are computed per sample.

**Benefits**: Stabilizes training, enables deeper networks.

---

## Optimizations

### FlashAttention (2022)

**Dao et al.** | [arXiv](https://arxiv.org/abs/2205.14135)

**Key idea**: Reduce memory usage of attention by tiling computation.

**Benefits**: Faster training, enables longer sequences, reduces memory from $O(n^2)$ to $O(n)$.

---

### Multi-Query Attention (MQA)

**Key idea**: Share keys and values across heads, only queries are separate.

**Benefits**: Reduces memory and computation while maintaining quality.

Used in: LLaMA, PaLM, modern LLMs.

---

## Vision Transformer Variants

**DeiT**: Efficient training with distillation

**Swin**: Hierarchical, window-based attention

**PVT (Pyramid Vision Transformer)**: Multi-scale feature pyramid

**PVTv2**: Improved PVT with overlapping patch embedding

---

## Connections to 3D Vision

**Multi-view transformers**: Use transformer attention to fuse multi-view features (DETR3D, PETR).

**BEV methods**: Transformers aggregate features from multiple cameras to BEV space (BEVFormer).

**Attention in NeRF**: Some methods use attention for view-dependent effects.

---

## Related Modules

- Module 1.1: Self-Supervised Learning (DINO uses ViT)
- Module 1.2: Multimodal Models (CLIP uses ViT encoder)
- Module 8.1: Multi-View Transformers
- Module 8.2: BEV Models

---

## Additional Resources

- **The Illustrated Transformer**: [jalammar.github.io/illustrated-transformer](https://jalammar.github.io/illustrated-transformer/)
- **An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale**: Original ViT paper
- **timm library**: PyTorch image models, includes many ViT variants

---

<div style="text-align: center; margin-top: 2em;">
[‚Üê Back to Index](../index.md.html) | [Previous: Classical CV ‚Üê](00-foundations-classical-cv.md.html)
</div>

</code>
</body>
</html>
