<!DOCTYPE html>
<meta charset="utf-8" emacsmode="-*- markdown -*-">
<link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/journal.css?">
<style>
/* Burgundy color theme override */
.md h1, .md h2, .md h3, .md h4, .md h5, .md h6 { color: #8B1538; }
.md a { color: #8B1538; }
.md a:hover { color: #A0446C; }
</style>

<!-- Markdeep: https://casual-effects.com/markdeep/ -->

# Self-Supervised Learning

**Module 1.1** | [‚Üê Back to Index](../index.html) | [Next: Multimodal Foundation Models ‚Üí](01-representation-multimodal.md.html)

---

## Overview

Self-supervised learning: Learn visual representations without human labels. Contrastive learning, masked image modeling, and other pretext tasks enable learning from unlabeled data.

---

## Essential Papers

### üî• SimCLR (2020)

**A Simple Framework for Contrastive Learning** (Chen et al.) | [arXiv](https://arxiv.org/abs/2002.05709)

**Key idea**: Contrastive learning with data augmentation.

**Framework**:
1. Apply random augmentations to image (two views)
2. Encode both views with CNN
3. Maximize similarity of augmented pairs
4. Minimize similarity with other images

**Loss**: InfoNCE loss:
$$L = -\log \frac{\exp(\text{sim}(z_i, z_j) / \tau)}{\sum_{k \neq i} \exp(\text{sim}(z_i, z_k) / \tau)}$$

**Components**:
- Strong data augmentation crucial
- Large batch sizes help
- Non-linear projection head improves representations

[Figure placeholder: SimCLR framework diagram]

---

### üî• MoCo (2020)

**Momentum Contrast for Unsupervised Visual Representation Learning** (He et al.) | [arXiv](https://arxiv.org/abs/1911.05722)

**Key innovation**: Momentum encoder + memory bank for large negatives.

**Momentum encoder**: Moving average of main encoder, more stable.

**Memory bank**: Stores features from past batches, enables many negatives without large batches.

**Benefits**: Works with smaller batches, better representations.

---

### üî• BYOL (2020)

**Bootstrap Your Own Latent** (Grill et al.) | [arXiv](https://arxiv.org/abs/2006.07733)

**Key innovation**: Self-supervised learning without negatives.

**Architecture**: Two networks (online and target), online predicts target.

**Process**: Online network predicts augmented view, target provides targets (moving average).

**Benefits**: No negative sampling needed, simpler.

**Surprising**: Works without collapse (still not fully understood).

---

### üî• DINO (2021)

**Emerging Properties in Self-Supervised Vision Transformers** (Caron et al.) | [arXiv](https://arxiv.org/abs/2104.14294)

**Key idea**: Self-distillation with vision transformers.

**Process**: Teacher and student ViTs, teacher provides soft targets.

**Benefits**: Learns segmentation-like attention, strong features.

**Use case**: Foundation features for many vision tasks.

---

### üî• DINOv2 (2024)

**DINOv2: Learning Robust Visual Features without Supervision** (Oquab et al.) | [arXiv](https://arxiv.org/abs/2304.07193)

**Improvements over DINO**:
- Larger dataset (142M images)
- Better training recipe
- Stronger features

**Benefits**: Strong general-purpose features, good for downstream tasks.

---

### MAE (2021)

**Masked Autoencoders Are Scalable Vision Learners** (He et al.) | [arXiv](https://arxiv.org/abs/2111.06377)

**Key idea**: Mask random image patches, predict masked patches.

**Process**:
1. Mask ~75% of image patches
2. Encode visible patches with ViT
3. Decode to reconstruct masked patches

**Benefits**: Simple, scalable, strong features.

---

## Core Concepts

### Contrastive Learning

**Goal**: Learn representations where similar samples are close, dissimilar are far.

**Positive pairs**: Different views of same image (augmentations).

**Negative pairs**: Different images.

**Loss**: Contrastive loss pushes positives together, negatives apart.

---

### Data Augmentation

**Critical component**: Strong augmentations essential for good representations.

**Common augmentations**: Random crop, color jitter, blur, flip.

**Augmentation policy**: Stronger augmentations ‚Üí better generalization.

---

### Non-Linear Projection

**Projection head**: Small MLP on top of encoder.

**Purpose**: Separate representation learning from contrastive learning.

**Removal**: Projection head removed after training, use encoder features.

---

## Problems Solved by Self-Supervised Learning

### Learning from Unlabeled Data
**Problem**: Supervised learning requires expensive manual labeling. For vision, ImageNet-scale labeling costs millions of dollars and thousands of person-hours.

**Self-supervised solution**: Learn representations from unlabeled data using pretext tasks (predicting rotations, solving jigsaw puzzles) or contrastive learning. Leverages vast amounts of unlabeled images available on internet.

[Figure placeholder: Comparison showing supervised learning (labeled images ‚Üí expensive) vs self-supervised (unlabeled images ‚Üí free)]

### Transfer Learning Foundation
**Problem**: Training models from scratch for each task is inefficient. Need reusable representations that transfer across tasks.

**Solution**: Self-supervised pre-training learns general-purpose features. Fine-tune on downstream tasks with small labeled datasets. Enables learning with limited labels.

### Better Representations
**Problem**: Supervised learning can produce task-specific features that don't generalize well. Features may focus on dataset-specific biases.

**Self-supervised solution**: Learns more general, robust features by avoiding task-specific supervision. Better generalization to new tasks and domains.

### Scaling with Data
**Problem**: Labeled datasets limited by annotation cost. Can't scale to internet-scale data.

**Self-supervised solution**: Can use billions of unlabeled images. Scales better with data. DINOv2 uses 142M images, would be impractical to label.

---

## Remaining Challenges and Limitations

### Evaluation and Metrics
**Problem**: Measuring quality of self-supervised representations is indirect - need to evaluate on downstream tasks. No direct quality metric.

**Current solutions**: Linear probing, fine-tuning evaluation, but still indirect and task-dependent.

**Open question**: Better metrics for representation quality? Theoretical understanding?

### Theoretical Understanding
**Problem**: Why does contrastive learning work so well? What features does it learn? Theoretical foundations still incomplete.

**Open question**: Better theoretical understanding? Guarantees about learned features?

### Computational Cost
**Problem**: Large-scale self-supervised training (e.g., DINOv2 with 142M images) requires massive compute resources.

**Current solutions**: More efficient architectures, better training strategies, but still expensive.

**Open question**: Can we achieve similar quality with less compute? More efficient algorithms?

### Task-Specific vs General Features
**Problem**: Self-supervised features are general but may not be optimal for specific tasks. Fine-tuning helps but may forget general features.

**Open question**: Better balance between general and task-specific features? Transfer without forgetting?

### Domain Generalization
**Problem**: Features learned on natural images (ImageNet, web images) may not transfer well to specialized domains (medical, satellite, etc.).

**Open question**: Better domain generalization? Self-supervised learning for specialized domains?

### Bias and Fairness
**Problem**: Self-supervised models learn biases from training data. Web-scraped data contains societal biases.

**Open question**: Better bias mitigation? Fairness in self-supervised learning?

---

## Broader Insights and Implications

### The Power of Unlabeled Data
**Insight**: Self-supervised learning demonstrates that vast amounts of unlabeled data can be leveraged effectively. Labels aren't always necessary.

**Broader impact**: Enables learning from internet-scale datasets. Reduces dependence on expensive annotation. Opens up new data sources.

### Pretext Tasks Reveal Structure
**Insight**: Simple pretext tasks (predicting rotation, solving jigsaw) force models to learn useful features. The task design guides what features are learned.

**Broader impact**: Shows that task design matters. Better pretext tasks ‚Üí better features. Encourages exploration of novel pretext tasks.

### Contrastive Learning as Universal Framework
**Insight**: Contrastive learning (learn by comparing) appears to be general principle. Works across modalities (images, text, audio, 3D).

**Broader impact**: Unified framework for learning representations. Influences design of methods across domains.

### The Importance of Data Augmentation
**Insight**: Self-supervised learning reveals critical importance of data augmentation. Strong augmentations essential for good representations.

**Broader impact**: Highlights that augmentation design is crucial. Better augmentations can improve representations more than architectural changes.

### Foundation Models Paradigm
**Insight**: Self-supervised pre-training creates foundation models. Single model can be used for many downstream tasks.

**Broader impact**: Enables foundation model paradigm. Pre-train once, use for many tasks. More efficient than task-specific training.

### Emergent Properties
**Insight**: DINO shows that self-supervised learning can produce unexpected emergent properties (attention maps that segment objects).

**Broader impact**: Suggests that powerful representations may have emergent properties we don't explicitly train for. Encourages exploration of what models learn.

[Placeholder for manual expansion: Add insights about impact on computer vision, democratization of AI, connections to biological learning]

---

## Applications

- Pre-training for downstream tasks
- Feature extraction
- Image retrieval
- Segmentation (DINO attention)
- Foundation for other models (CLIP uses self-supervised features)

[Figure placeholder: Applications diagram showing self-supervised pre-training ‚Üí downstream tasks: classification, detection, segmentation, retrieval, etc.]

---

## Related Modules

- Module 0.3: Transformers (DINO uses ViT)
- Module 1.2: Multimodal Foundation Models (builds on self-supervised features)
- Module 8.1: Multi-View Transformers (uses learned features)

---

## Additional Resources

- **DINOv2 Repository**: Official implementation
- **Self-Supervised Learning Survey**: Comprehensive overview

---

---
[‚Üê Back to Index](../index.html) | [Next: Multimodal Foundation Models ‚Üí](01-representation-multimodal.md.html)
---


<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="markdeep.min.js" charset="utf-8"></script><script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js" charset="utf-8"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
